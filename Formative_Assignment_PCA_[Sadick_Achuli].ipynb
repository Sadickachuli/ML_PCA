{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sadickachuli/ML_PCA/blob/main/Formative_Assignment_PCA_%5BSadick_Achuli%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "PCA is influenced by the variance of the features. Features with bigger scales can improperly affect the findings so it becomes important to treat the data on the same scale such that each feature has a mean of 0 and a standard deviation of 1. This also guarantees that every feature contributes equally to the analysis, making it possible for PCA to more precisely determine the directions of highest variance. This results in a more balanced analysis, enhanced performance, and easier interpretation of the PCA results.\n"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# USing the formula above\n",
        "mean = np.mean(data, axis=0)\n",
        "standard_dev = np.std(data, axis=0)\n",
        "standardized_data = (data - mean) / standard_dev\n",
        "print(standardized_data)\n",
        "\n",
        "# A more easier way would have been to import the sklearn.preprocessing package and used this formula below\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# standardized_data = scaler.fit_transform(data)\n",
        "# print(standardized_data)"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9103478-ff22-484f-9be8-cc2050786b92"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "2427c446-604d-455f-934d-d04570ec30fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "print(cov_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Eigenvalues:\\n\", eigenvalues)\n",
        "print(\"\\nEigenvectors:\\n\", eigenvectors)\n"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba568d9d-c64d-4752-cd18-71153319d28a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            " [3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "\n",
            "Eigenvectors:\n",
            " [[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "e8d4af36-c5ac-4127-d368-dcbc69d7746e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "The reason why we rank the eigenvalues and eigenvectors is to find the principal components that most accurately represent the variation in the data. The components with the highest importance can be identified by arranging the eigenvalues in descending order. We may identify the directions of greatest variance in the data by aligning the eigenvectors with these sorted eigenvalues. This guarantees that the principal components are arranged according to their importance.\n",
        "\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "No. In fact, we took into account the highest eigenvalues rather than the lowest ones during my brief PCA course. Which basically means that principal components with higher eigenvalues capture more variance in the data and are therefore more significant for dimensionality reduction. Lower eigenvalues on the other hand may indicate less important patterns and noise since they capture less variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "explained_variance = sorted_eigenvalues / np.sum(sorted_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "27f7825f-1d09-4892-9296-f9b13ed5481f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2\n",
        "reduced_data = np.matmul(standardized_data, sorted_eigenvectors[:, :k])"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "d12d0167-8ea0-40ea-c69f-fdaa852cbed8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.3577116  -0.75728867]\n",
            " [-2.27171739 -1.81970663]\n",
            " [ 1.21259114 -0.50390931]\n",
            " [-1.41935914  1.9229856 ]\n",
            " [ 1.61562536  0.87541857]\n",
            " [-1.49485157  0.28250044]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "bd56e0da-053d-43b3-aaa5-0f6169484899"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "\n",
        "Two Benefis of PCA:\n",
        "1. Visualization: The most obvious reason why we use PCA is to reduce the dimensions in the dataset while retaining most of the variance.  This enables the visualization of high-dimensional data, which aids in the identification of outliers, patterns, and clusters that are not obvious in the original feature space.\n",
        "\n",
        "2. Noise Reduction: As I indicated while responding to the question of whether we should prioritize the lowest eigen value above the highest, noise exists in data. By concentrating on the principal components that capture the most variation, PCA can reduce noise in the data by possibly removing less useful and noisy components.\n",
        "\n",
        "\n",
        "Two Limitations of PCA:\n",
        "1. Loss of some valuable information: One downside about PCA is that, while reducing the noise to retain most of the variance in a data, it actually loses some part of the data.  For some tasks, the eliminated variance might contain some information that could result in a loss of predicted accuracy.\n",
        "\n",
        "2. Loss of Interpretability: The principal components following PCA are just linear combinations of the initial features. This reduces the model's interpretability by making it difficult to understand the primary components in terms of the original features.\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCIJqcYv9wmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}